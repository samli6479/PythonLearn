Big Data Processing

MapReduce is a framework for batch processing of big data

Framework: A system used by programmers to build applications

Batch processing: All the data is avaliable at the outset, and results aren't used until processing completes

Big Data: Used to describe data sets to large and comprehensive that they can reveal facts about a whole population, usually from statistical analysis

The MapReduce idea:

Data sets are too big to be analyzed by one machine

Using multiple machines has the same complications, regardless of the application/analysis

Map phase: Apply a mapper function to all inputs, emitting intermediate key-value pairs

 - The mapper takes an iterable value containing inputs, such as lines of text

 - The mapper yields zero or more key-value pairs for each input





